nohup: ignoring input
[10/20/2025 23:03:08 gluefactory INFO] Starting experiment sp+lg_selfda_homography
[10/20/2025 23:03:13 gluefactory INFO] Training in distributed mode with 2 GPUs
[10/20/2025 23:03:13 gluefactory INFO] Training in distributed mode with 2 GPUs
[10/20/2025 23:03:14 gluefactory INFO] Using device 0
[10/20/2025 23:03:14 gluefactory INFO] Using device 1
[10/20/2025 23:03:21 gluefactory.datasets.base_dataset INFO] Creating dataset HomographyDataset
[10/20/2025 23:03:21 gluefactory.datasets.base_dataset INFO] Creating dataset HomographyDataset
[10/20/2025 23:03:28 gluefactory.datasets.homographies INFO] Found 1001001 images in list file.
[10/20/2025 23:03:29 gluefactory.datasets.homographies INFO] Found 1001001 images in list file.
/gemini/final/gluefactory/datasets/augmentations.py:130: UserWarning: Argument(s) 'always_apply' are not valid for transform FromFloat
  self.preprocess = A.FromFloat(always_apply=True, dtype="uint8")
/gemini/final/gluefactory/datasets/augmentations.py:131: UserWarning: Argument(s) 'always_apply' are not valid for transform ToFloat
  self.postprocess = A.ToFloat(always_apply=True)
/gemini/final/gluefactory/datasets/augmentations.py:130: UserWarning: Argument(s) 'always_apply' are not valid for transform FromFloat
  self.preprocess = A.FromFloat(always_apply=True, dtype="uint8")
/gemini/final/gluefactory/datasets/augmentations.py:131: UserWarning: Argument(s) 'always_apply' are not valid for transform ToFloat
  self.postprocess = A.ToFloat(always_apply=True)
[10/20/2025 23:03:29 gluefactory INFO] Training loader has 1171 batches
[10/20/2025 23:03:29 gluefactory INFO] Validation loader has 32 batches
[10/20/2025 23:03:32 gluefactory INFO] Parameters with scaled learning rate:
{}
[10/20/2025 23:03:32 gluefactory INFO] Training with mixed_precision=None
No fused RMSNorm
[10/20/2025 23:03:32 gluefactory INFO] Parameters with scaled learning rate:
{}
[10/20/2025 23:03:32 gluefactory INFO] Training with mixed_precision=None
[10/20/2025 23:03:32 gluefactory INFO] Starting training with configuration:
data:
  name: homographies
  data_dir: revisitop1m
  train_size: 150000
  val_size: 2000
  batch_size: 128
  num_workers: 14
  homography:
    difficulty: 0.7
    max_angle: 45
  photometric:
    name: lg
model:
  name: two_view_pipeline
  extractor:
    name: gluefactory_nonfree.superpoint
    max_num_keypoints: 512
    force_num_keypoints: true
    detection_threshold: 0.0
    nms_radius: 3
    trainable: false
  ground_truth:
    name: matchers.homography_matcher
    th_positive: 3
    th_negative: 3
  matcher:
    name: matchers.lightglue_selfda
    filter_threshold: 0.1
    flash: false
    checkpointed: true
train:
  seed: 0
  epochs: 40
  optimizer: adam
  opt_regexp: null
  optimizer_options: {}
  lr: 0.0001
  lr_schedule:
    type: exp
    start: 20
    exp_div_10: 10
    on_epoch: true
    factor: 1.0
    options: {}
  lr_scaling:
  - - 100
    - - dampingnet.const
  eval_every_iter: 500
  save_every_iter: 5000
  log_every_iter: 100
  log_grad_every_iter: null
  test_every_epoch: 1
  keep_last_checkpoints: 10
  load_experiment: null
  median_metrics: []
  recall_metrics: {}
  pr_metrics: {}
  best_key: loss/total
  dataset_callback_fn: null
  dataset_callback_on_val: false
  clip_grad: null
  pr_curves: {}
  plot:
  - 5
  - gluefactory.visualization.visualize_batch.make_match_figures
  submodules: []
benchmarks:
  hpatches:
    eval:
      estimator: opencv
      ransac_th: 0.5
'True': null

[10/20/2025 23:03:32 gluefactory INFO] Starting epoch 0
No fused RMSNorm
Premature end of JPEG file
[10/20/2025 23:05:51 gluefactory INFO] [E 0 | it 0] loss {total 8.046E+00, last 7.205E+00, assignment_nll 7.205E+00, nll_pos 1.380E+01, nll_neg 6.050E-01, num_matchable 1.968E+02, num_unmatchable 3.038E+02, confidence 8.554E-01, row_norm 5.536E-01}
Evaluation:   0%|          | 0/32 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb90c2b72e0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1618, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1582, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 145204) is killed by signal: Aborted. 
Evaluation:   0%|          | 0/32 [01:57<?, ?it/s]
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f7629ab72e0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1618, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1582, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 145201) is killed by signal: Aborted. 
[rank0]:[W1020 23:07:51.328799667 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1020 23:07:53.088000 142193 site-packages/torch/multiprocessing/spawn.py:169] Terminating process 142282 via signal SIGTERM
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/gemini/final/gluefactory/train.py", line 732, in <module>
    torch.multiprocessing.spawn(
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 215, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
    fn(i, *args)
  File "/gemini/final/gluefactory/train.py", line 667, in main_worker
    training(rank, conf, output_dir, args)
  File "/gemini/final/gluefactory/train.py", line 571, in training
    results, pr_metrics, figures = do_evaluation(
                                   ^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gemini/final/gluefactory/train.py", line 92, in do_evaluation
    pred = model(data)
           ^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gemini/final/gluefactory/models/base_model.py", line 114, in forward
    return self._forward(data)
           ^^^^^^^^^^^^^^^^^^^
  File "/gemini/final/gluefactory/models/two_view_pipeline.py", line 81, in _forward
    pred = {**pred, **self.matcher({**data, **pred})}
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gemini/final/gluefactory/models/matchers/lightglue_selfda.py", line 639, in forward
    desc0, desc1 = self.transformers[i](desc0, desc1)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/glue_factory/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: TransformerLayer.forward() missing 2 required positional arguments: 'encoding0' and 'encoding1'

